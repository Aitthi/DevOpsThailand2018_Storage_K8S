Instruction for Setup Heketi/Gluster (Assumption: Kubernetes Cluster Already Provide)
====================================================
Environment Description: 
NAME             STATUS    ROLES     AGE       VERSION
ip-10-21-1-128   Ready     master    13m       v1.11.2
ip-10-21-1-188   Ready     <none>    1m        v1.11.2
ip-10-21-1-27    Ready     <none>    2m        v1.11.2
ip-10-21-1-68    Ready     <none>    5m        v1.11.2
===================================================
################################################################# Gluster/Heketi Setup ###############################################################################
0. (All Worker Node) Install kernel load module by command:
sudo modprobe dm_snapshot       ==> Check by "sudo lsmod |grep dm_snapshot"
sudo modprobe dm_mirror         ==> Check by "sudo lsmod |grep dm_mirror"
sudo modprobe dm_thin_pool      ==> Check by "sudo lsmod |grep dm_thin_pool"

Device Disk on device: /dev/xvdg on Worker 1/2/3
1. (All Worker Node) Setup Gluster Client by Command:
sudo apt-get install -y software-properties-common
sudo add-apt-repository ppa:gluster/glusterfs-4.0
sudo apt-get install -y glusterfs-client

2. (Master) Git Clone repository
git clone https://github.com/praparn/DevOpsThailand2018_Storage_K8S.git   ==> This is source for all lab
git clone https://github.com/gluster/gluster-kubernetes.git

3. (Master) Edit topology.json and copy
vi ./DevOpsThailand2018_Storage_K8S/topology.json     ==> Change all hostname/ip address to actual host 1, 2, 3
{
  "clusters": [
    {
      "nodes": [
        {
          "node": {
            "hostnames": {
              "manage": [
                "ip-10-21-1-68"
              ],
              "storage": [
                "10.21.1.68"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/xvdg"
          ]
        },
        {
          "node": {
            "hostnames": {
              "manage": [
                "ip-10-21-1-27"
              ],
              "storage": [
                "10.21.1.27"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/xvdg"
          ]
        },
        {
          "node": {
            "hostnames": {
              "manage": [
                "ip-10-21-1-188"
              ],
              "storage": [
                "10.21.1.188"
              ]
            },
            "zone": 1
          },
          "devices": [
            "/dev/xvdg"
          ]
        }
      ]
    }
  ]
}
cp ./DevOpsThailand2018_Storage_K8S/topology.json ./gluster-kubernetes/deploy/topology.json


4. (Master) Access to deployment and chmod for execute to "gk-deploy" 
cd ./gluster-kubernetes/deploy/
sudo chmod +x ./gk-deploy

5. (Master) Update python binary and Operate gk-cluster by command:
sudo apt-get update
sudo apt install -y python-minimal
sudo ./gk-deploy -g 
======================================================================================================================================
ubuntu@iip-10-21-1-128:~/gluster-kubernetes/deploy$ ./gk-deploy -g
Welcome to the deployment tool for GlusterFS on Kubernetes and OpenShift.

Before getting started, this script has some requirements of the execution
environment and of the container platform that you should verify.

The client machine that will run this script must have:
 * Administrative access to an existing Kubernetes or OpenShift cluster
 * Access to a python interpreter 'python'

Each of the nodes that will host GlusterFS must also have appropriate firewall
rules for the required GlusterFS ports:
 * 2222  - sshd (if running GlusterFS in a pod)
 * 24007 - GlusterFS Management
 * 24008 - GlusterFS RDMA
 * 49152 to 49251 - Each brick for every volume on the host requires its own
   port. For every new brick, one new port will be used starting at 49152. We
   recommend a default range of 49152-49251 on each host, though you can adjust
   this to fit your needs.

The following kernel modules must be loaded:
 * dm_snapshot
 * dm_mirror
 * dm_thin_pool

For systems with SELinux, the following settings need to be considered:
 * virt_sandbox_use_fusefs should be enabled on each node to allow writing to
   remote GlusterFS volumes

In addition, for an OpenShift deployment you must:
 * Have 'cluster_admin' role on the administrative account doing the deployment
 * Add the 'default' and 'router' Service Accounts to the 'privileged' SCC
 * Have a router deployed that is configured to allow apps to access services
   running in the cluster

Do you wish to proceed with deployment?

[Y]es, [N]o? [Default: Y]: Y 
Using Kubernetes CLI.
Using namespace "default".
Checking for pre-existing resources...
  GlusterFS pods ... not found.
  deploy-heketi pod ... not found.
  heketi pod ... not found.
  gluster-s3 pod ... not found.
Creating initial resources ... serviceaccount "heketi-service-account" created
clusterrolebinding "heketi-sa-view" created
clusterrolebinding "heketi-sa-view" labeled
OK
node "ip-10-21-1-170" labeled
node "ip-10-21-1-232" labeled
node "ip-10-21-1-216" labeled
daemonset "glusterfs" created
Waiting for GlusterFS pods to start ... OK
secret "heketi-config-secret" created
secret "heketi-config-secret" labeled
service "deploy-heketi" created
deployment "deploy-heketi" created
Waiting for deploy-heketi pod to start ... OK
Creating cluster ... ID: 284a58c78c93911497d759c6dd5875e3
Allowing file volumes on cluster.
Allowing block volumes on cluster.
Creating node ip-10-21-1-170 ... ID: fb840adefec4178cfbf706dda8a7bbf0
Adding device /dev/xvdg ... OK
Creating node ip-10-21-1-232 ... ID: 05b07c3a8b509b3b1324df58f16269a4
Adding device /dev/xvdg ... OK
Creating node ip-10-21-1-216 ... ID: f4233961decbc803da84417f1cf83e48
Adding device /dev/xvdg ... OK
heketi topology loaded.
Saving /tmp/heketi-storage.json
secret "heketi-storage-secret" created
endpoints "heketi-storage-endpoints" created
service "heketi-storage-endpoints" created
job "heketi-storage-copy-job" created

======================================================================================================================================

6. (Master) Setup Heketi Cli:
   Heketi Setup
   wget https://github.com/heketi/heketi/releases/download/v7.0.0/heketi-v7.0.0.linux.amd64.tar.gz
   tar xzvf heketi-v7.0.0.linux.amd64.tar.gz
   cd heketi
   sudo cp heketi-cli /usr/local/bin/
   cd ..
  

7. (Master) Test Heketi-Cli by command:
  kubectl get pods
  kubectl get svc

 Test the service
  curl http://<cluster-ip>:8080/hello
  heketi-cli -s http://<cluster-ip>:8080 cluster list
  export HEKETI_CLI_SERVER=http://<address to heketi service>:8080

 Test create volume
  heketi-cli cluster list
  heketi-cli cluster info <cluster id>
  heketi-cli node info <node id>
  heketi-cli volume create --size=2
  heketi-cli volume list
  heketi-cli volume info <volume id>
  heketi-cli volume delete <volume id>

8. (Master) Test create stroage class for claim glusterfs volume by command:
Taint master node:
kubectl taint nodes <Master Name> dedicated=admin:NoSchedule

Dynamically Provision a Volume
   Create a StorageClass 
   # cd ./DevOpsThailand2018_Storage_K8S/ && vi Storageclass.yaml ==> add cluster ip of heketi server

   # kubectl create -f Storageclass.yaml

   # kubectl get sc

   Create a PersistentVolumeClaim (PVC)
   
   # kubectl create -f gluster-pvc.yaml

   # kubectl get pvc

   # kubectl get pv

  (Master)
  export HEKETI_CLI_SERVER=http://<address to heketi service>:8080
  heketi-cli volume list
  
Create a NGINX pod that uses the PVC
   # kubectl create -f nginx.yaml

   # kubectl get pods -o wide

   exec into the container and create an index.html file
   # kubectl exec -ti nginx-pod1 /bin/sh
   $ cd /usr/share/nginx/html
   $ echo 'Hello World from GlusterFS!!!' > index.html
   $ ls
   index.html
   $ exit

   # curl http://<ip of pods>
   Hello World from GlusterFS!!!


9. (Master) Clean Up lab
kubectl delete -f nginx.yaml
kubectl delete -f gluster-pvc.yaml
kubectl delete -f Storageclass.yaml


==============================================================Install prometheus monitoring===========================================================================

git clone https://github.com/camilb/prometheus-kubernetes.git
cd prometheus-kubernetes
./deploy